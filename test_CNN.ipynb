{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "from sympy.combinatorics import Permutation\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# local data_dir\n",
    "data_dir = Path(\"data\")\n",
    "\n",
    "# Load puzzle info\n",
    "puzzle_info = pd.read_csv(data_dir / 'puzzle_info.csv', index_col='puzzle_type')\n",
    "# Parse allowed_moves\n",
    "puzzle_info['allowed_moves'] = puzzle_info['allowed_moves'].apply(literal_eval)\n",
    "# puzzle_info_df['total_allowed_moves'] = puzzle_info_df['allowed_moves_dict'].apply(lambda x: len(x.keys()))\n",
    "puzzle_info['number_moves'] = puzzle_info['allowed_moves'].apply(lambda x: len(x.keys()))\n",
    "\n",
    "# Load puzzles\n",
    "puzzles_all = pd.read_csv(data_dir / 'puzzles.csv', index_col='id')\n",
    "# Parse color states\n",
    "puzzles_all = puzzles_all.assign(\n",
    "    initial_state=lambda df: df['initial_state'].str.split(';'),\n",
    "    solution_state=lambda df: df['solution_state'].str.split(';')\n",
    ")\n",
    "# keep only puzzles with puzzle_type in cube_3/3/3\n",
    "puzzles_all = puzzles_all[puzzles_all['puzzle_type'] == 'cube_3/3/3']\n",
    "\n",
    "puzzles_all['total_components'] = puzzles_all['solution_state'].apply(len)\n",
    "puzzles_all['all_unique_components'] = puzzles_all['solution_state'].apply(lambda x: np.unique(x))\n",
    "puzzles_all['unique_components'] = puzzles_all['all_unique_components'].apply(len)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['f0', 'f1', 'f2', 'r0', 'r1', 'r2', 'd0', 'd1', 'd2', '-f0', '-f1', '-f2', '-r0', '-r1', '-r2', '-d0', '-d1', '-d2']\n"
     ]
    }
   ],
   "source": [
    "# create ditionary of allowed moves\n",
    "allowed_moves = puzzle_info.loc['cube_3/3/3']['allowed_moves']\n",
    "# create dictionary of allowed moves converting list to np.array\n",
    "allowed_moves = {k: np.array(v) for k, v in allowed_moves.items()}\n",
    "# include inverse moves\n",
    "move_key = list(allowed_moves.keys())\n",
    "for i in range(len(allowed_moves)):\n",
    "    move = allowed_moves[move_key[i]]\n",
    "    allowed_moves[f\"-{move_key[i]}\"] = np.argsort(move)\n",
    "\n",
    "# update move_key\n",
    "move_key = list(allowed_moves.keys())\n",
    "\n",
    "print(move_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "def preprocess_cube_state(state, encoder):\n",
    "    '''\n",
    "    Preprocess a cube state for input into the CNN\n",
    "    Args:\n",
    "    state: 1D array of cube state (3 x 3 x 6 = 54 labels)\n",
    "    encoder: previously fit OneHotEncoder object\n",
    "    \n",
    "    Return: 3D array of one-hot encoded cube state\n",
    "    '''\n",
    "    # Reshape the state into a 3x3x6 array (6 faces, each 3x3)\n",
    "    reshaped_state = state.reshape(6, 3, 3)\n",
    "\n",
    "    # One-hot encode the labels\n",
    "    one_hot_encoded = encoder.transform(reshaped_state.reshape(-1, 1)).toarray()\n",
    "\n",
    "    # Reshape back to 3D structure suitable for CNN (each label is now a one-hot vector)\n",
    "    one_hot_reshaped = one_hot_encoded.reshape(6, 3, 3, -1)\n",
    "\n",
    "    return one_hot_reshaped\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_move(state, move):\n",
    "    \"\"\"\n",
    "    Apply a move to the cube state.\n",
    "\n",
    "    Args:\n",
    "    state (np.array): The current state of the cube.\n",
    "    move (str): The move to apply.\n",
    "    allowed_moves (dict): Dictionary of moves and their corresponding permutation functions.\n",
    "\n",
    "    Returns:\n",
    "    np.array: New state of the cube after the move.\n",
    "    \"\"\"\n",
    "    return state[move]\n",
    "\n",
    "import random\n",
    "\n",
    "def generate_random_state(solution_state, allowed_moves, used_states, min_moves=3, max_moves=20):\n",
    "    \"\"\"\n",
    "    Generate a random cube state by applying a series of random moves to the solution state.\n",
    "    Checks against a set of used states to avoid duplicates.\n",
    "\n",
    "    Args:\n",
    "    solution_state (np.array): The solution state of the cube.\n",
    "    allowed_moves (dict): Dictionary of moves and their corresponding permutation functions.\n",
    "    used_states (set): A set of states already used/generated.\n",
    "    min_moves (int): Minimum number of random moves.\n",
    "    max_moves (int): Maximum number of random moves.\n",
    "\n",
    "    Returns:\n",
    "    np.array: A randomly manipulated state of the cube, or None if a duplicate is found.\n",
    "    list: The list of moves applied to reach this state, or None if a duplicate is found.\n",
    "    int: The number of moves applied, or None if a duplicate is found.\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        num_moves = random.randint(min_moves, max_moves)\n",
    "        move_sequence = random.choices(list(allowed_moves.keys()), k=num_moves)\n",
    "\n",
    "        current_state = solution_state.copy()\n",
    "        for move in move_sequence:\n",
    "            current_state = apply_move(current_state, allowed_moves[move])\n",
    "\n",
    "        state_tuple = tuple(current_state.flatten())  # Convert to tuple for set operations\n",
    "        if state_tuple not in used_states:\n",
    "            used_states.add(state_tuple)\n",
    "            return current_state, move_sequence, num_moves\n",
    "        # If the state is a duplicate, the loop continues to generate a new sequence\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of moves: 3\n",
      "Move sequence: ['r0', '-f1', '-r2']\n",
      "Random state:\n",
      "['D' 'A' 'B' 'D' 'C' 'C' 'D' 'A' 'B' 'A' 'B' 'F' 'C' 'B' 'F' 'A' 'B' 'F'\n",
      " 'C' 'D' 'C' 'C' 'F' 'C' 'C' 'F' 'C' 'A' 'D' 'F' 'A' 'D' 'E' 'A' 'D' 'F'\n",
      " 'E' 'E' 'E' 'A' 'A' 'B' 'E' 'E' 'E' 'B' 'F' 'D' 'B' 'E' 'E' 'B' 'F' 'D']\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# get solution state\n",
    "solution_state = np.array(puzzles_all.iloc[30]['solution_state'])\n",
    "\n",
    "# Create a set of used states to avoid duplicates\n",
    "used_states = set()\n",
    "\n",
    "# Example of generating a unique state and its associated move count\n",
    "random_state, move_sequence, num_moves = generate_random_state(solution_state, allowed_moves, \n",
    "                                                               used_states, min_moves=3, max_moves=20)\n",
    "\n",
    "used_states.add(tuple(solution_state.flatten()))  # Add the solution state to avoid generating it\n",
    "\n",
    "\n",
    "print(f\"Number of moves: {num_moves}\")\n",
    "print(f\"Move sequence: {move_sequence}\")\n",
    "print(f\"Random state:\\n{random_state}\")\n",
    "print(type(random_state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing 10000 of 300000\n",
      "writing 20000 of 300000\n",
      "writing 30000 of 300000\n",
      "writing 40000 of 300000\n",
      "writing 50000 of 300000\n",
      "writing 60000 of 300000\n",
      "writing 70000 of 300000\n",
      "writing 80000 of 300000\n",
      "writing 90000 of 300000\n",
      "writing 100000 of 300000\n",
      "writing 110000 of 300000\n",
      "writing 120000 of 300000\n",
      "writing 130000 of 300000\n",
      "writing 140000 of 300000\n",
      "writing 150000 of 300000\n",
      "writing 160000 of 300000\n",
      "writing 170000 of 300000\n",
      "writing 180000 of 300000\n",
      "writing 190000 of 300000\n",
      "writing 200000 of 300000\n",
      "writing 210000 of 300000\n",
      "writing 220000 of 300000\n",
      "writing 230000 of 300000\n",
      "writing 240000 of 300000\n",
      "writing 250000 of 300000\n",
      "writing 260000 of 300000\n",
      "writing 270000 of 300000\n",
      "writing 280000 of 300000\n",
      "writing 290000 of 300000\n",
      "writing 300000 of 300000\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "\n",
    "# Define all possible labels in the cube states\n",
    "all_possible_labels = np.array(['A', 'B', 'C', 'D', 'E', 'F']).reshape(-1, 1)\n",
    "# Fit the OneHotEncoder with all possible labels\n",
    "encoder = OneHotEncoder()\n",
    "encoder.fit(all_possible_labels)\n",
    "\n",
    "# Main script to generate the dataset\n",
    "num_samples = 300000\n",
    "batch_size = 10000  # Save data every 10000 samples\n",
    "used_states = {tuple(solution_state.flatten())}  # Initialize used states with the solution state\n",
    "\n",
    "# Initialize lists to store data\n",
    "states = []\n",
    "move_counts = []\n",
    "used_states = set()\n",
    "\n",
    "# File to save the data\n",
    "h5_file = h5py.File('cube_data.h5', 'w')\n",
    "\n",
    "for i in range(num_samples):\n",
    "    random_state, move_sequence, num_moves = generate_random_state(solution_state, allowed_moves, \n",
    "                                                                   used_states, min_moves=3, max_moves=20)\n",
    "    \n",
    "    # peform preprocessing on random_state\n",
    "    random_state = preprocess_cube_state(random_state, encoder)\n",
    "    \n",
    "    # Convert random_state to a 1D array and store\n",
    "    states.append(random_state.flatten())\n",
    "    move_counts.append(num_moves)\n",
    "\n",
    "    # Update the used_states set\n",
    "    used_states.add(tuple(random_state.flatten()))\n",
    "\n",
    "    # Save and clear lists in batches to reduce memory usage\n",
    "    if (i + 1) % batch_size == 0 or (i + 1) == num_samples:\n",
    "        print(f\"writing {i + 1} of {num_samples}\")\n",
    "        # Convert lists to numpy arrays for saving\n",
    "        states_array = np.array(states)\n",
    "        move_counts_array = np.array(move_counts)\n",
    "\n",
    "        # Save the batch to file\n",
    "        h5_file.create_dataset(f'states_batch_{i // batch_size}', data=states_array)\n",
    "        h5_file.create_dataset(f'move_counts_batch_{i // batch_size}', data=move_counts_array)\n",
    "\n",
    "        # Clear lists for the next batch\n",
    "        states.clear()\n",
    "        move_counts.clear()\n",
    "\n",
    "# Close the file\n",
    "h5_file.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(324,)\n",
      "[1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      " 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      " 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0.\n",
      " 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      " 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      " 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      " 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0.\n",
      " 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      " 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "h5_file = h5py.File('cube_data.h5', 'r')\n",
    "states_batch = h5_file[f'states_batch_{i}'][:]\n",
    "move_counts_batch = h5_file[f'move_counts_batch_{i}'][:]\n",
    "\n",
    "for state in states_batch:\n",
    "    print(state.shape)\n",
    "    print(state)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 3, 3, 6)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def reshape_data(flattened_data, n_categories):\n",
    "    \"\"\"\n",
    "    Reshape the flattened one-hot encoded data back to its original 3D structure.\n",
    "\n",
    "    Args:\n",
    "    flattened_data (np.array): The flattened one-hot encoded data.\n",
    "    n_categories (int): Number of categories used in one-hot encoding.\n",
    "\n",
    "    Returns:\n",
    "    np.array: Reshaped data into its original 3D structure.\n",
    "    \"\"\"\n",
    "    # Calculate the total number of elements in one face\n",
    "    face_elements = 3 * 3 * n_categories\n",
    "\n",
    "    # Reshape back to 3D structure (6 faces, 3x3, with one-hot encoded labels)\n",
    "    reshaped_data = flattened_data.reshape((6, 3, 3, n_categories))\n",
    "\n",
    "    return reshaped_data\n",
    "\n",
    "reshape_data(state, 6).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data(h5_file):\n",
    "    all_states = []\n",
    "    all_move_counts = []\n",
    "\n",
    "    #for i in range(h5_file.attrs['num_batches']):\n",
    "    for i in range(2):\n",
    "        # Load data\n",
    "        states_batch = h5_file[f'states_batch_{i}'][:]\n",
    "        move_counts_batch = h5_file[f'move_counts_batch_{i}'][:]\n",
    "\n",
    "        # Decode and preprocess states\n",
    "        for state in states_batch:\n",
    "            decoded_state = reshape_data(state, n_categories=6)  # Implement this function based on your encoding scheme\n",
    "            #preprocessed_state = preprocess_cube_state(decoded_state)\n",
    "            all_states.append(decoded_state)\n",
    "\n",
    "        all_move_counts.extend(move_counts_batch)\n",
    "\n",
    "    return np.array(all_states), np.array(all_move_counts)\n",
    "\n",
    "# Open the HDF5 file and load the data\n",
    "h5_file = h5py.File('cube_data.h5', 'r')\n",
    "states, move_counts = load_and_preprocess_data(h5_file)\n",
    "h5_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 6, 3, 3, 6)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the goal of your CNN is to predict the number of moves away from the goal state for a given cube state, you should treat this as a regression problem. In a regression setting, the CNN will output a continuous value representing the estimated number of moves, rather than classifying the input into categories.\n",
    "\n",
    "Here's how you can adapt the CNN architecture, compilation, and training process for a regression task:\n",
    "\n",
    "## 1. Define the CNN Architecture for Regression\n",
    "For regression, the output layer will have a single neuron (since you're predicting a single continuous value - the number of moves), and you will typically use a linear activation function (or no activation function) for the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv3D, MaxPooling3D, Flatten, Dense, Dropout\n",
    "\n",
    "model = Sequential([\n",
    "    Conv3D(32, (3, 3, 3), activation='relu', padding='same', input_shape=(6, 3, 3, 6)),\n",
    "    MaxPooling3D((2, 2, 2)),\n",
    "    Conv3D(64, (3, 3, 3), activation='relu', padding='same'),\n",
    "    #MaxPooling3D((2, 2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1)  # Single neuron for regression output\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Compile the Model for Regression\n",
    "When compiling the model for a regression task, use a loss function suitable for regression, like Mean Squared Error (MSE).\n",
    "\n",
    "## 3. Train the Model\n",
    "Train your model on the states and their corresponding move counts. Ensure that move_counts contains numerical values (not categorical labels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 6, 3, 3, 6)\n",
      "(20000,)\n"
     ]
    }
   ],
   "source": [
    "print(states.shape)\n",
    "print(move_counts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "500/500 [==============================] - 2s 3ms/step - loss: 25.0137 - val_loss: 13.6258\n",
      "Epoch 2/10\n",
      "500/500 [==============================] - 1s 3ms/step - loss: 15.0463 - val_loss: 12.2275\n",
      "Epoch 3/10\n",
      "500/500 [==============================] - 1s 3ms/step - loss: 13.5416 - val_loss: 12.3655\n",
      "Epoch 4/10\n",
      "500/500 [==============================] - 1s 3ms/step - loss: 12.6567 - val_loss: 12.1925\n",
      "Epoch 5/10\n",
      "500/500 [==============================] - 1s 3ms/step - loss: 12.1162 - val_loss: 12.2655\n",
      "Epoch 6/10\n",
      "500/500 [==============================] - 2s 3ms/step - loss: 11.6584 - val_loss: 12.4624\n",
      "Epoch 7/10\n",
      "500/500 [==============================] - 1s 3ms/step - loss: 11.2832 - val_loss: 12.9302\n",
      "Epoch 8/10\n",
      "500/500 [==============================] - 1s 3ms/step - loss: 10.9220 - val_loss: 12.6262\n",
      "Epoch 9/10\n",
      "500/500 [==============================] - 2s 3ms/step - loss: 10.8025 - val_loss: 12.8418\n",
      "Epoch 10/10\n",
      "500/500 [==============================] - 2s 3ms/step - loss: 10.4417 - val_loss: 12.9888\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='mean_squared_error')\n",
    "\n",
    "history = model.fit(states, move_counts, epochs=10, validation_split=0.2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluate the Model\n",
    "Evaluate the model using a regression metric like Mean Squared Error or Mean Absolute Error on a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of states_test: (5000, 6, 3, 3, 6)\n",
      "shape of move_counts_test: (5000,)\n"
     ]
    }
   ],
   "source": [
    "# create test set\n",
    "# Initialize np.arrays to store test data\n",
    "states_test\n",
    "\n",
    "\n",
    "states_test = []\n",
    "move_counts_test = []\n",
    "used_states_test = set()\n",
    "\n",
    "\n",
    "for i in range(5000):\n",
    "    random_state, move_sequence, num_moves = generate_random_state(solution_state, allowed_moves, \n",
    "                                                                   used_states, min_moves=3, max_moves=20)\n",
    "    \n",
    "    # peform preprocessing on random_state\n",
    "    random_state = preprocess_cube_state(random_state, encoder)\n",
    "    \n",
    "    # Convert random_state to a 1D array and store\n",
    "    states_test.append(random_state)\n",
    "    move_counts_test.append(num_moves)\n",
    "\n",
    "    # Update the used_states set\n",
    "    used_states_test.add(tuple(random_state.flatten()))\n",
    "\n",
    "\n",
    "states_test = np.array(states_test)\n",
    "move_counts_test = np.array(move_counts_test)\n",
    "\n",
    "print(f\"shape of states_test: {states_test.shape}\")\n",
    "print(f\"shape of move_counts_test: {move_counts_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 0s 912us/step - loss: 13.2794\n",
      "Test Loss (Mean Squared Error): 13.279394149780273\n"
     ]
    }
   ],
   "source": [
    "test_loss = model.evaluate(states_test, move_counts_test)\n",
    "print(f\"Test Loss (Mean Squared Error): {test_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "2023_12_KagglePolytopePerm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
